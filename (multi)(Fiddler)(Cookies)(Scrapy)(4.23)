from multiprocessing.dummy import Pool
pool = Pool(4) # 核心數
results = pool.map(爬蟲函數,網址列表)

""" Fiddler introduce """
PC <-> Fiddler <->service
         |
         v
        copy
        
WinConfig -> IE -> visit website -> look data


""" Cookies introduce """
keep sign in 

------------------------------------------
Fiddler obtain cookies

cookie = {'Cookie': 'XXXXXX'}
html = requests.get(url,cookies = cookie)




'''  Scrapy introduce '''
install list:
lxml
zope.interface
Twisted
pyOpenSSL
pywin32
Scrapy

-------------
Scrapy core code 
scrapy startproject xxx

import scarpy
from scrapy.contrib.spiders import CrawlSpider
from scrapy.http import Request
from scrapy.seclector import Selector
xxx = selector.xpath(xxx).extract

ex:
from scarpy.contrib.spiders import CrawlSpider

class Douban(CrawlSpider):
  name = "Test"
  start_urls = ['website local']
  def parse(self,response):
    pirnt(response.body)
    
    
ltem.py 定義需要抓取並須要後期處理的數據
settings.py 文件配置Scrapy,從而修改user-agent, 設定爬取時間間隔,設置代理,配置各種中間件等等。
pipeline.py 用於存放執行後期數據處理的功能,從而使得數據的爬取和處理分開


